{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNa6GndiC2P8x5Gfqjk9+fj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"Nnl1StCwU1vD","executionInfo":{"status":"ok","timestamp":1699664588753,"user_tz":300,"elapsed":7580,"user":{"displayName":"Thomas Schaeffer","userId":"16924689648810531872"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader, TensorDataset"]},{"cell_type":"code","source":["# Load CIFAR-100\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","])\n","\n","trainset = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n","testset = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)\n","\n","trainset = DataLoader(trainset, batch_size=64, shuffle=True)\n","testset = DataLoader(testset, batch_size=64, shuffle=False)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8fd26N3-0UHa","executionInfo":{"status":"ok","timestamp":1699664597419,"user_tz":300,"elapsed":8672,"user":{"displayName":"Thomas Schaeffer","userId":"16924689648810531872"}},"outputId":"a09ff3f1-60e5-4004-aab6-825d3c91cc89"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 169001437/169001437 [00:01<00:00, 97099800.95it/s] \n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/cifar-100-python.tar.gz to ./data\n","Files already downloaded and verified\n"]}]},{"cell_type":"code","source":["# Model\n","class CNN(nn.Module):\n","    def __init__(self):\n","\n","        super(CNN, self).__init__()\n","        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n","\n","        self.convolution_1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n","        self.convolution_2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n","        self.convolution_3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n","\n","        self.linear_layer_1 = nn.Linear(128 * 4 * 4, 512)\n","        self.linear_layer_2 = nn.Linear(512, 100)\n","\n","\n","    def forward(self, x):\n","        x = self.pool(nn.functional.relu(self.convolution_1(x)))\n","        x = self.pool(nn.functional.relu(self.convolution_2(x)))\n","        x = self.pool(nn.functional.relu(self.convolution_3(x)))\n","        x = x.view(-1, 128 * 4 * 4)\n","        x = nn.functional.relu(self.linear_layer_1(x))\n","        x = self.linear_layer_2(x)\n","        return x"],"metadata":{"id":"fEIIkJLK0YQX","executionInfo":{"status":"ok","timestamp":1699664597422,"user_tz":300,"elapsed":25,"user":{"displayName":"Thomas Schaeffer","userId":"16924689648810531872"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# Initialize model\n","model = CNN()\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","# Put model in GPU\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","model = model.to(device)\n","\n","# Train model\n","epochs = 30\n","for epoch in range(epochs):\n","\n","    model.train()\n","\n","    for i, (images, labels) in enumerate(trainset):\n","\n","        images = images.to(device)\n","        labels = labels.to(device)\n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","\n","    print(f'Epoch [{epoch + 1}]')\n","    print(f'Loss: {loss.item():.4f}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"imhUqrS80QlQ","outputId":"83612ae9-2c8c-474c-f679-c81dc75d957a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [1]\n","Loss: 2.8258\n","Epoch [2]\n","Loss: 1.7600\n","Epoch [3]\n","Loss: 1.8401\n","Epoch [4]\n","Loss: 2.0146\n","Epoch [5]\n","Loss: 1.9149\n","Epoch [6]\n","Loss: 1.7809\n","Epoch [7]\n","Loss: 1.5063\n","Epoch [8]\n","Loss: 1.0980\n","Epoch [9]\n","Loss: 0.8968\n","Epoch [10]\n","Loss: 1.0871\n","Epoch [11]\n","Loss: 0.9639\n","Epoch [12]\n","Loss: 0.5508\n","Epoch [13]\n","Loss: 0.2935\n","Epoch [14]\n","Loss: 0.6133\n","Epoch [15]\n","Loss: 0.2655\n","Epoch [16]\n","Loss: 0.2447\n","Epoch [17]\n","Loss: 0.3271\n","Epoch [18]\n","Loss: 0.4064\n","Epoch [19]\n","Loss: 0.0848\n","Epoch [20]\n","Loss: 0.0452\n","Epoch [21]\n","Loss: 0.0257\n","Epoch [22]\n","Loss: 0.2009\n","Epoch [23]\n","Loss: 0.1878\n","Epoch [24]\n","Loss: 0.0428\n","Epoch [25]\n","Loss: 0.2624\n","Epoch [26]\n","Loss: 0.0056\n","Epoch [27]\n","Loss: 0.1480\n","Epoch [28]\n","Loss: 0.3767\n","Epoch [29]\n","Loss: 0.1630\n"]}]},{"cell_type":"code","source":["# Evaluate and test\n","\n","accuracy = 0\n","model.eval()\n","predictions_array = []\n","labels_array = []\n","\n","for i in range(1):\n","  for j, batch in enumerate(testset):\n","\n","    images, labels = batch\n","    images = images.to(device)\n","    labels = labels.to(device)\n","    outputs = model(images)\n","    _, predictions = torch.max(outputs, 1)\n","\n","    # Outputs\n","    predictions_array += predictions\n","    labels_array += labels\n","\n","# Initialize\n","correlation_counter = 0\n","samples = len(predictions_array)\n","\n","# Determine correlated outputs\n","for i in range(len(predictions_array)):\n","  if predictions_array[i].item() == labels_array[i].item():\n","    correlation_counter += 1\n","temporary_accuracy = correlation_counter/samples\n","\n","# Print accuracy\n","if temporary_accuracy > accuracy:\n","  accuracy = tmp\n","  print(accuracy)"],"metadata":{"id":"W-B5KCla0RJE"},"execution_count":null,"outputs":[]}]}